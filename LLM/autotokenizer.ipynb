{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoTokenizer\n",
    "\n",
    "Hugging Face의 `AutoTokenizer`는 다양한 자연어 처리(NLP) 모델을 위한 자동화된 토크나이제이션 클래스이다.\n",
    "\n",
    "사용자가 모델에 맞는 적절한 토크나이저를 선택하면 모델의 이름이나 경로에 따라 적절한 토크나이저를 자동으로 로드하여 다양한 모델과 함께 사용할 수 있는 장점이 있다.\n",
    "\n",
    "Auto: 사용자의 편의를 위해 다양한 토크나이저와 모델을 자동으로 관리하고 선택할 수 있게 해주는 기능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AutoTokenizer.from_pretrained`을 통해 원하는 모델의 토크나이저를 불러 올 수 잇음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Google-Bert\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불러온 토크나이저를 통한 텍스트 토크나이징\n",
    "\n",
    "Tokenization의 결과로 input_ids, token_type_ids, attention_mask의 딕셔너리를 얻음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2079, 2025, 19960, 10362, 1999, 1996, 3821, 1997, 16657, 1010, 2005, 2027, 2024, 11259, 1998, 4248, 2000, 4963, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = bert_tokenizer(\"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\")\n",
    "\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer.tokenize`는 주어진 텍스트를 토큰으로 분리함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['do', 'not', 'med', '##dle', 'in', 'the', 'affairs', 'of', 'wizards', '.']\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.tokenize(\"Do not meddle in the affairs of wizards.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer.vocab_size` : 토크나이저의 vocab_size를 확인할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`encode`와 `decode`를 통해 토큰화와 문장화를 수행할 수 있음<br>\n",
    "영어를 기반으로 학습된 토크나이저로 한글에 대한 토크나이징 성능은 떨어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ᄒ', '##ᅡ', '##ᆫ', '##ᄀ', '##ᅳ', '##ᆯ', '##ᄋ', '##ᅳ', '##ᆫ', '[UNK]', '[UNK]', '?']\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.tokenize(\"한글은 어떻게 될까요?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 한글은 [UNK] [UNK]? [SEP]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input = bert_tokenizer.encode(\"한글은 어떻게 될까요?\")\n",
    "bert_tokenizer.decode(encoded_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
