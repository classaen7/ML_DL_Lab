{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding (BPE)\n",
    "\n",
    "Subword Tokenizer란 텍스트 데이터를 하위 단어(subword)로 분리하여 처리하는 tokenization 기법\n",
    "\n",
    "BPE는 자주 등장하는 문자 쌍을 반복적으로 병합하며, 점점 더 큰 단위(sub word)를 만들어냄 <br>\n",
    "병합이 종료되면 고정된 크기의 subword 사전이 생성되고, 이 사전을 통해 텍스트를 토큰으로 나눌 수 있음\n",
    "\n",
    "### Training Tokenizer\n",
    "\n",
    "1. training corpus에 존재하는 모든 문자를 토큰으로 변환하여 시작\n",
    "2. 가장 자주 등장하는 토큰 쌍을 찾아 하나의 토큰으로 병합\n",
    "3. 원하는 크기의 (fixed)vocab_size에 도달할 때까지 반복\n",
    "\n",
    "### Training BPE in Huggingface \n",
    "1. Initialization\n",
    "\n",
    "먼저 BPE 모델을 사용하여 Tokenizer 객체를 생성함<br>\n",
    "pre-tokenizer를 whitespace로 설정<br>\n",
    ": 토크나이저가 BPE를 적용하기 전에 처음에는 공백을 기준으로 텍스트를 분할함 <br>\n",
    "\n",
    "2. Training\n",
    "\n",
    "vocab_size, min_frequency(어휘에 포함되기 위해 토큰이 가져야 할 최소 빈도)와 같은 특정 매개변수로 BpeTrainer를 정의<br>\n",
    "Trainer에 데이터셋을 제공하여 학습 시킴\n",
    "\n",
    "\n",
    "3. Tokenization\n",
    "\n",
    "훈련 후, `encode`를 사용하여 텍스트 토크나이즈를 수행함<br>\n",
    "출력 : 토크나이즈된 하위 단어, 해당 ID <br>\n",
    "`decode`도 가능하지만 완벽한 재구성이 아닐 수 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a BPE tokenizer\n",
    "tokenizer = Tokenizer(BPE())\n",
    "# pre-tokenzier는 Whitespace : 공백을 기준으로 초기 설정\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# BpeTrainer 매개 변수 설정\n",
    "trainer = BpeTrainer(vocab_size=1000, min_frequency=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training data\n",
    "data = [\"Hello, how are you?\", \"I am fine, thank you.\", \"How about you?\"]\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train_from_iterator(data, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습에 사용한 데이터 중 빈번하게 발생하는 하위 단어 'ow', 'you'를 확인할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['H', 'e', 'l', 'l', 'o', ',', 'h', 'ow', 'a', 'r', 'e', 'you', '?']\n",
      "Input IDs: [3, 7, 12, 12, 15, 0, 9, 23, 5, 16, 7, 22, 2]\n",
      "Decoded text: H e l l o , h ow a r e you ?\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a new sentence\n",
    "output = tokenizer.encode(\"Hello, how are you?\")\n",
    "print(\"Tokens:\", output.tokens)\n",
    "\n",
    "# Convert tokens to IDs\n",
    "print(\"Input IDs:\", output.ids)\n",
    "\n",
    "# Decode IDs back to text\n",
    "print(\"Decoded text:\", tokenizer.decode(output.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=13, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습에 사용하지 않은 텍스트의 경우 토크나이징이 이뤄지지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=0, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tokenizer.encode(\"안녕하세요\")\n",
    "print(\"Tokens:\", output.tokens)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습용 데이터를 불러와 토크나이저를 학습할 수 있음\n",
    "\n",
    "또한 학습된 토크나이저는 `tokenzier.save`를 통해 json 파일로 저장하여 재사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokens: ['H', 'e', 'l', 'l', 'o', 'o', 'w', 'a', 'r', 'e', 'y', 'o', 'u', '?']\n",
      "IDs: [5, 7, 8, 8, 9, 9, 12, 6, 10, 7, 13, 9, 11, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ᄋ', 'ᅡ', 'ᆫ', 'ᄂ', 'ᅧ', 'ᆼ', 'ᄒ', 'ᅡ', 'ᄉ', 'ᅦ', '요']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, StripAccents\n",
    "\n",
    "# Initialize a BPE tokenizer\n",
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Normalize text (optional but recommended)\n",
    "tokenizer.normalizer = normalizers.Sequence([NFD(), StripAccents()])\n",
    "\n",
    "# Define a trainer for the tokenizer\n",
    "trainer = BpeTrainer(vocab_size=10000, min_frequency=2, special_tokens=[\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"])\n",
    "\n",
    "# Train the tokenizer using an input text file\n",
    "files = [\"test.txt\"]\n",
    "tokenizer.train(files, trainer)\n",
    "\n",
    "# Save the tokenizer model for future use\n",
    "tokenizer.save(\"bpe_tokenizer.json\")\n",
    "\n",
    "# Test the tokenizer on new text\n",
    "encoded = tokenizer.encode(\"Hello, how are you?\")\n",
    "print(\"Tokens:\", encoded.tokens)\n",
    "print(\"IDs:\", encoded.ids)\n",
    "\n",
    "tokenizer.encode(\"안녕하세요\").tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
