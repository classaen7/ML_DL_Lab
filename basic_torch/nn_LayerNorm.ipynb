{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Normalization\n",
    "\n",
    "Layer Normalization은 딥러닝 모델에서 각 층의 출력을 정규화하는 기법으로, 훈련 안정성과 수렴 속도를 개선하는 데 도움을 주는 기법이다.\n",
    "\n",
    "단어 그대로 각 층을 Normalize한다고 보면 된다.\n",
    "\n",
    "Layer Normalization에서 중요한 것은 `어디`와 `어떻게`이다.\n",
    "\n",
    "먼저 `어떻게`에 대해 알아보겠다.<br>\n",
    "정규화 수식은 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sigma} \\cdot \\gamma + \\beta\n",
    "$$\n",
    "\n",
    "정규화 대상인 입력 `x`에 대하여 평균과 표준편차를 사용하여 정규화를 수행한다.<br>\n",
    "정규화가 된 경우 평균이 0, 표준편차가 1이 되는 정규분포의 형태를 띄게 된다.<br>\n",
    "\n",
    "이렇게 정규화를 수행한 뒤, $\\gamma$와 $\\beta$로 affine transform을 수행한다.<br>\n",
    "이에 대해 생각해보면 먼저 정규분포의 형태로 데이터를 정규화한 뒤, 아핀 변환으로 분포를 데이터와 태스크에 적합하도록 조정하는 것이다.<br>\n",
    "단순히 정규화만 수행하는게 아니라는 것이 중요하다.\n",
    "\n",
    "코드로 `nn.LayerNorm`을 확인해보면 weight와 bias가 나오는데, 각각 $\\gamma$와 $\\beta$를 의미한다.<br>\n",
    "`nn.LayerNorm` 객체를 불러오고 약간의 조작을 통해 연산을 확인해보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.,  4.,  6.,  8., 10.], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.tensor([0.2, 0.2, 0.2, 0.2, 0.2])\n",
    "\n",
    "ln = nn.LayerNorm(5)\n",
    "# ln.weight.data = torch.tensor([1., 2., 3., 4., 5.], requires_grad=True)\n",
    "\n",
    "# 직관적인 확인을 위해 beta 값 만을 변화시켜 확인하겠다.\n",
    "ln.bias.data = torch.tensor([2.,4.,6.,8.,10.], requires_grad=True)\n",
    "\n",
    "ln(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 `어디`에 대해 알아보겠다.<br>\n",
    "각 층을 대상으로 정규화라고 하였는데, '층'이라는 표현은 다소 모호하다. <br>\n",
    "\n",
    "LayerNorm은 보통 RNN, transformer 모델에서 적용한다.<br>\n",
    "따라서 입력 텐서의 크기가 `(Batch, Sequence_length, hidden_dim)`라고 가정하겠다.<br>\n",
    "이때 각 단어별 벡터를 대상으로 정규화를 수행한다.<br>\n",
    "따라서 `nn.LayerNorm`은 `hidden_dim`의 크기를 할당하여 객체를 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.9128e-05, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 배치 크기, 시퀀스 길이, 히든 차원\n",
    "batch, seq_len, h_dim = 3, 12, 768\n",
    "input_tensor = torch.rand(batch, seq_len, h_dim)\n",
    "\n",
    "layer_norm = nn.LayerNorm(normalized_shape=h_dim)\n",
    "\n",
    "ln_tensor = layer_norm(input_tensor)\n",
    "print(sum(ln_tensor[0,0,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이때 `normalized_shape`에 오는 크기는 입력 차원의 뒤에서부터 순서대로 와야한다.<br>\n",
    "LayerNorm의 대상을 정하는 것이므로 이미지 텐서를 예시로 들면 (열, 행, 차원, 배치) 순서로 정규화 차원이 확장되는 것이다.<br>\n",
    "코드로 자세히 확인하겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "N, C, H, W = 2, 3, 4, 4  \n",
    "input_tensor = torch.rand(N, C, H, W)  \n",
    "\n",
    "# Layer Normalization을 위한 nn.LayerNorm 모듈\n",
    "layer_norm_1 = nn.LayerNorm(normalized_shape=W)\n",
    "layer_norm_2 = nn.LayerNorm(normalized_shape=(H, W))\n",
    "layer_norm_3 = nn.LayerNorm(normalized_shape=(C, H, W))\n",
    "layer_norm_4 = nn.LayerNorm(normalized_shape=(N, C, H, W))\n",
    "\n",
    "# 각 Layer Normalization을 적용 수행\n",
    "output_tensor = layer_norm_1(input_tensor)\n",
    "output_tensor = layer_norm_2(input_tensor)\n",
    "output_tensor = layer_norm_3(input_tensor)\n",
    "output_tensor = layer_norm_4(input_tensor)\n",
    "\n",
    "# 오류 없이 수행되는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 4]) torch.Size([3, 4, 4])\n",
      "torch.Size([2, 3, 4, 4]) torch.Size([2, 3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "print(layer_norm_3.weight.data.shape,\n",
    "layer_norm_3.bias.data.shape)\n",
    "\n",
    "print(layer_norm_4.weight.data.shape,\n",
    "layer_norm_4.bias.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수식 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0793, -1.4627, -0.0971, -0.1949,  1.6753]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "ln = nn.LayerNorm(5)\n",
    "input = torch.randn(1, 5)\n",
    "\n",
    "output = ln(input)\n",
    "print(output)\n",
    "print(ln.weight)\n",
    "print(ln.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0793, -1.4627, -0.0971, -0.1949,  1.6753]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = input.mean(-1, keepdim=True)\n",
    "var = (input - mean).pow(2).mean(-1, keepdim=True)\n",
    "norm = (input - mean) / torch.sqrt(var + ln.eps) * ln.weight + ln.bias\n",
    "\n",
    "print(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.LayerNorm과의 비교\n",
    "torch.allclose(output, norm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
